{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import main packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage.color import rgb2hsv\n",
    "from skimage.morphology import closing, opening, disk\n",
    "from skimage import io\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pdob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_images(file_path, save_path, size=(1500, 1000)):\n",
    "    im = Image.open(file_path)\n",
    "    downsampled_image = im.resize(size)  # Specify desired width and height\n",
    "\n",
    "    # Create the save directory if it does not exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # Construct the save path for the downsampled image\n",
    "    filename = os.path.basename(file_path)\n",
    "    downsampled_image_save_path = os.path.join(save_path, filename)\n",
    "    \n",
    "    # Save the downsampled image\n",
    "    downsampled_image.save(downsampled_image_save_path)\n",
    "\n",
    "def downsample_images_in_directory(input_directory, output_directory, size=(1500, 1000)):\n",
    "    # List all JPG images in the directory\n",
    "    images = glob.glob(os.path.join(input_directory, '*.JPG'))\n",
    "    \n",
    "    for image_path in images:\n",
    "        downsample_images(image_path, output_directory, size)\n",
    "        print(f\"Processed and saved: {os.path.basename(image_path)}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/3. hand\"\n",
    "output_directory = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/hand_downsampled\"\n",
    "#downsample_images_in_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_background(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    threshold_noisy = ((image[:,:,0] > 60) & (image[:,:,0] < 130))\n",
    "    threshold_hand = (hsv_image[:,:,0] > 125)\n",
    "\n",
    "    if (np.sum(threshold_noisy)) > 1000000:\n",
    "        background = \"noisy\"\n",
    "       \n",
    "    elif np.sum(threshold_hand) > 500000:\n",
    "        background = \"hand\"\n",
    "    \n",
    "    else:\n",
    "        background = \"neutral\"\n",
    "        \n",
    "    print(background)\n",
    "    return background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_display_circles_hand(img, display=False):\n",
    "    # Load the image\n",
    "    \"\"\"img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        print(\"Error: Image not found at\", image_path)\n",
    "        return None\"\"\"\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian Blur to the image\n",
    "    blurred = cv2.GaussianBlur(gray, (9, 9), 2)\n",
    "\n",
    "    edges = cv2.Canny(blurred, 15, 60, apertureSize=3)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=90, minLineLength=1, maxLineGap=80)\n",
    "    mask = np.ones_like(edges) * 255\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1, y1, x2, y2 in line:\n",
    "                cv2.line(mask, (x1, y1), (x2, y2), 0, 5)\n",
    "\n",
    "    edges = cv2.bitwise_and(edges, mask)\n",
    "    mask_watch = np.ones_like(edges) * 255\n",
    "    # Mask for the watch\n",
    "    cv2.rectangle(mask_watch, (0, 850), (1500, 1000), 0, -1)\n",
    "    edges = cv2.bitwise_and(edges, mask_watch)\n",
    "\n",
    "    # Apply Hough Circle Transform on the masked image\n",
    "    circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, dp=1, minDist=100,\n",
    "                               param1=150, param2=22, minRadius=50, maxRadius=100)\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if circles is not None:\n",
    "            circles = np.uint16(np.around(circles))[0, :]\n",
    "            for (x, y, r) in circles:\n",
    "                plt.gca().add_patch(plt.Circle((x, y), r, color='red', fill=False, linewidth=2))\n",
    "        plt.title('Detected Circles in ')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    if circles is not None:\n",
    "        return [(x, y, r) for (x, y, r) in np.uint16(np.around(circles))[0, :]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def detect_and_display_circles_neutral(img, display=False):\n",
    "    # Load the image\n",
    "    # img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    # if img is None:\n",
    "    #    print(\"Error: Image not found at\", image_path)\n",
    "    #    return None\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian Blur\n",
    "    blur = cv2.GaussianBlur(gray, (9, 9), 2)\n",
    "\n",
    "    # Apply adaptive thresholding and morphological operations\n",
    "    img_th = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    kernel = np.ones((7, 7), np.uint8)\n",
    "    closing = cv2.morphologyEx(img_th, cv2.MORPH_CLOSE, kernel)\n",
    "    closing = cv2.dilate(closing, kernel, iterations=3)\n",
    "\n",
    "    # Hough Circle Transform\n",
    "    circles = cv2.HoughCircles(closing, cv2.HOUGH_GRADIENT, dp=1, minDist=100,\n",
    "                               param1=50, param2=10, minRadius=50, maxRadius=120)\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if circles is not None:\n",
    "            circles = np.uint16(np.around(circles))[0, :]\n",
    "            for (x, y, r) in circles:\n",
    "                plt.gca().add_patch(plt.Circle((x, y), r, color='red', fill=False, linewidth=2))\n",
    "        plt.title('Detected Circles')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    if circles is not None:\n",
    "        return [(x, y, r) for (x, y, r) in np.uint16(np.around(circles))[0, :]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def detect_and_display_circles_noisy(img, display=False):\n",
    "    # img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    data_h, data_s, data_v = cv2.split(hsv)\n",
    "    img_th = np.zeros(data_s.shape, dtype=np.uint8)\n",
    "    img_th[(data_s > 0.4 * 255) & (data_h < 0.12 * 180)] = 255\n",
    "    # kernel = np.ones((7, 7), np.uint8)  # Define the kernel size\n",
    "    # img_th = cv2.morphologyEx(img_th, cv2.MORPH_CLOSE, kernel)\n",
    "    # plt.imshow(img_th)\n",
    "\n",
    "    # Apply GaussianBlur to reduce image noise and detail\n",
    "    blur = cv2.GaussianBlur(img_th, (9, 9), 2)\n",
    "\n",
    "    # Detect circles in the image using HoughCircles\n",
    "    \"\"\"\"\n",
    "    mindist = distance entre 2 cercles :/ => tricky\n",
    "    param 1 = higher threshold of canny detector => the higher the less edges it detects, bien pour ne pas detecter les unnecessary edges\n",
    "    param 2 = the smaller it is the more circles it dtects, including false circles\n",
    "\n",
    "    \"\"\"\n",
    "    circles = cv2.HoughCircles(blur, cv2.HOUGH_GRADIENT, dp=1.2, minDist=50, param1=50, param2=40, minRadius=30,\n",
    "                               maxRadius=100)\n",
    "\n",
    "    # Convert the (x, y) coordinates and radius of the circles to integers\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))[0, :]\n",
    "\n",
    "    # Plot the image with circles\n",
    "    if display:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if circles is not None:\n",
    "        #    circles = np.uint16(np.around(circles))[0, :]\n",
    "            for (x, y, r) in circles:\n",
    "                plt.gca().add_patch(plt.Circle((x, y), r, color='red', fill=False, linewidth=2))\n",
    "        plt.title('Detected Circles')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    if circles is not None:\n",
    "        return [(x, y, r) for (x, y, r) in circles]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def crop_coins(image, circles):\n",
    "    cropped_images = []\n",
    "    for (x, y, r) in circles:\n",
    "        x, y, r = int(x), int(y), int(r)\n",
    "        cropped_img = image[y-r:y+r, x-r:x+r]\n",
    "        cropped_images.append(cropped_img)\n",
    "    return cropped_images\n",
    "\n",
    "\n",
    "def detect_circles_classification(img, display=False):\n",
    "    # Load the image\n",
    "    # img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    # if img is None:\n",
    "    #    print(\"Error: Image not found at\", image_path)\n",
    "    #    return None\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian Blur\n",
    "    blur = cv2.GaussianBlur(gray, (9, 9), 2)\n",
    "\n",
    "    # Apply adaptive thresholding and morphological operations\n",
    "    #img_th = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "    #                               cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    #kernel = np.ones((7, 7), np.uint8)\n",
    "    #closing = cv2.morphologyEx(img_th, cv2.MORPH_CLOSE, kernel)\n",
    "    #closing = cv2.dilate(closing, kernel, iterations=3)\n",
    "\n",
    "    # Hough Circle Transform\n",
    "    #circles = cv2.HoughCircles(closing, cv2.HOUGH_GRADIENT, dp=1, minDist=100,\n",
    "    #                           param1=60, param2=10, minRadius=50, maxRadius=120)\n",
    "    circles = cv2.HoughCircles(blur, cv2.HOUGH_GRADIENT, dp=1, minDist=100,\n",
    "                               param1=60, param2=10, minRadius=50, maxRadius=120)\n",
    "\n",
    "    if display:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if circles is not None:\n",
    "            circles = np.uint16(np.around(circles))[0, :]\n",
    "            for (x, y, r) in circles:\n",
    "                plt.gca().add_patch(plt.Circle((x, y), r, color='red', fill=False, linewidth=2))\n",
    "        plt.title('Detected Circles in')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    if circles is not None:\n",
    "        circles = np.uint16(np.around(circles))\n",
    "        if circles.shape[1] > 1:  # Check if there are multiple circles\n",
    "            (x, y, r) = circles[0, 0]  # Use the first detected circle\n",
    "        else:\n",
    "            (x, y, r) = circles[0, 0]\n",
    "\n",
    "        return (x, y, r)\n",
    "    else:\n",
    "        return (0, 0, 0)\n",
    "\n",
    "\n",
    "def detect_and_crop_coins(image_type, img=None, img_path=None, display_cropped=False,):\n",
    "    # Load the image\n",
    "    if img_path is not None:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    \"\"\"if img is None:\n",
    "        print(\"Error: Image not found at\", image_path)\n",
    "        return None\"\"\"\n",
    "\n",
    "    # Call the appropriate detection function\n",
    "    if image_type == 'neutral':\n",
    "        circles = detect_and_display_circles_neutral(img, display=False)\n",
    "    elif image_type == 'hand':\n",
    "        circles = detect_and_display_circles_hand(img, display=False)\n",
    "    elif image_type == 'noisy':\n",
    "        circles = detect_and_display_circles_noisy(img, display=False)\n",
    "    else:\n",
    "        print(\"Error: Unknown image type\")\n",
    "        return None\n",
    "\n",
    "    if not circles:\n",
    "        print(\"No circles detected\")\n",
    "        return []\n",
    "\n",
    "    # Crop the detected coins\n",
    "    cropped_images = crop_coins(img, circles)\n",
    "\n",
    "    # Display cropped images if required\n",
    "    if display_cropped:\n",
    "        for i, cropped_img in enumerate(cropped_images):\n",
    "            plt.figure(figsize=(4, 4))\n",
    "            plt.imshow(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f'Cropped Coin {i + 1}')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    return cropped_images, circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHF classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_area(area):\n",
    "    # 5\n",
    "    if area > 25000:\n",
    "        return 0\n",
    "    # 2\n",
    "    elif area > 17000:\n",
    "        return 1\n",
    "    # 1\n",
    "    elif area > 12000:\n",
    "        return 2\n",
    "    # 0.2\n",
    "    elif area > 10800:\n",
    "        return 4\n",
    "    # 0.1\n",
    "    elif area > 10000:\n",
    "        return 5\n",
    "    # 0.5\n",
    "    elif area > 9000:\n",
    "        return 3\n",
    "    # 0.05\n",
    "    else:\n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_area(circle):\n",
    "    (x, y, r) = circle\n",
    "    area = np.pi * r ** 2\n",
    "\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(outputs, labels):\n",
    "    preds = np.array(outputs)\n",
    "    labels = np.array(labels)\n",
    "    score = 0\n",
    "    for i, true_label in enumerate(labels):\n",
    "        TP = np.minimum(true_label, preds[i]).sum()\n",
    "        FPN = np.abs(true_label-preds[i]).sum()\n",
    "        if 2*TP + FPN != 0:\n",
    "            score += 2*TP/(2*TP + FPN)\n",
    "\n",
    "    return score/len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO ADAPT ###\n",
    "neutral_downsampled = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/neutral_downsampled\"\n",
    "noisy_downsampled = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/noisy_downsampled\"\n",
    "hand_downsampled = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/hand_downsampled\"\n",
    "\n",
    "neutral_folder = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/neutral\"\n",
    "noisy_folder = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/noisy\"\n",
    "hand_folder = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train/hand\"\n",
    "\n",
    "test_folder = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/test\"\n",
    "\n",
    "model_path1 = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr_group_55/project/classifier_wholetrainset_class3_b6_lr2e3_step7_epoch11_split0.75\"\n",
    "model_path2 = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr_group_55/project/resnet_b6_lr1e2_step7_epoch8_split0.75\"\n",
    "\n",
    "csv_file = \"C:/Users/rapha/OneDrive/Documents/2. EPFL/Master/MA2/Image Analysis & Pattern Recognition/iapr24-coin-counter/train_labels.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose folder to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO ADAPT ###\n",
    "folder1 = neutral_downsampled # ! Changer le background dans load_images_and_backgrounds\n",
    "folder2 = neutral_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model CHF/EUR/OOD (alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "model1 = torchvision.models.alexnet(weights='IMAGENET1K_V1')\n",
    "num_ftrs = model1.classifier[-1].in_features\n",
    "model1.classifier[-1] = nn.Linear(num_ftrs, 3)\n",
    "model1.load_state_dict(torch.load(model_path1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model EUR subclasses (resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model2 = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "num_ftrs = model2.fc.in_features\n",
    "model2.fc = nn.Linear(num_ftrs, 8)\n",
    "\n",
    "model2.load_state_dict(torch.load(model_path2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # Resize to match model's input size\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize pixel values, based on imagenet\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file L1010277.JPG\n",
      "Processing file L1010279.JPG\n",
      "Processing file L1010281.JPG\n",
      "Processing file L1010283.JPG\n",
      "Processing file L1010287.JPG\n",
      "Processing file L1010288.JPG\n",
      "Processing file L1010294.JPG\n",
      "Processing file L1010297.JPG\n",
      "Processing file L1010298.JPG\n",
      "Processing file L1010300.JPG\n",
      "Processing file L1010308.JPG\n",
      "Processing file L1010310.JPG\n",
      "Processing file L1010311.JPG\n",
      "Processing file L1010318.JPG\n",
      "Processing file L1010321.JPG\n",
      "Processing file L1010323.JPG\n",
      "Processing file L1010405.JPG\n",
      "Processing file L1010406.JPG\n",
      "Processing file L1010408.JPG\n",
      "Processing file L1010410.JPG\n",
      "Processing file L1010413.JPG\n",
      "Processing file L1010418.JPG\n",
      "Processing file L1010419.JPG\n",
      "Processing file L1010421.JPG\n",
      "Processing file L1010422.JPG\n",
      "Processing file L1010424.JPG\n",
      "Processing file L1010434.JPG\n",
      "Processing file L1010436.JPG\n",
      "Processing file L1010440.JPG\n",
      "Processing file L1010445.JPG\n",
      "Processing file L1010446.JPG\n",
      "Processing file L1010450.JPG\n",
      "Processing file L1010454.JPG\n",
      "Processing file L1010277.JPG\n",
      "neutral\n",
      "Processing file L1010279.JPG\n",
      "neutral\n",
      "Processing file L1010281.JPG\n",
      "neutral\n",
      "Processing file L1010283.JPG\n",
      "neutral\n",
      "Processing file L1010287.JPG\n",
      "neutral\n",
      "Processing file L1010288.JPG\n",
      "neutral\n",
      "Processing file L1010294.JPG\n",
      "neutral\n",
      "Processing file L1010297.JPG\n",
      "neutral\n",
      "Processing file L1010298.JPG\n",
      "neutral\n",
      "Processing file L1010300.JPG\n",
      "neutral\n",
      "Processing file L1010308.JPG\n",
      "neutral\n",
      "Processing file L1010310.JPG\n",
      "neutral\n",
      "Processing file L1010311.JPG\n",
      "neutral\n",
      "Processing file L1010318.JPG\n",
      "neutral\n",
      "Processing file L1010321.JPG\n",
      "neutral\n",
      "Processing file L1010323.JPG\n",
      "neutral\n",
      "Processing file L1010405.JPG\n",
      "neutral\n",
      "Processing file L1010406.JPG\n",
      "neutral\n",
      "Processing file L1010408.JPG\n",
      "neutral\n",
      "Processing file L1010410.JPG\n",
      "neutral\n",
      "Processing file L1010413.JPG\n",
      "neutral\n",
      "Processing file L1010418.JPG\n",
      "neutral\n",
      "Processing file L1010419.JPG\n",
      "neutral\n",
      "Processing file L1010421.JPG\n",
      "neutral\n",
      "Processing file L1010422.JPG\n",
      "neutral\n",
      "Processing file L1010424.JPG\n",
      "neutral\n",
      "Processing file L1010434.JPG\n",
      "neutral\n",
      "Processing file L1010436.JPG\n",
      "neutral\n",
      "Processing file L1010440.JPG\n",
      "neutral\n",
      "Processing file L1010445.JPG\n",
      "neutral\n",
      "Processing file L1010446.JPG\n",
      "neutral\n",
      "Processing file L1010450.JPG\n",
      "neutral\n",
      "Processing file L1010454.JPG\n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "def load_images_and_backgrounds(folder, downsampled=True):\n",
    "    images = []\n",
    "    ids = []\n",
    "    backgrounds = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.JPG'):\n",
    "                print(f\"Processing file {file}\")\n",
    "                # Construct path to the image file\n",
    "                file_path = os.path.join(root, file)\n",
    "                im = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "                img_id = file[:-4]\n",
    "\n",
    "                # Classify background\n",
    "                if not downsampled:\n",
    "                    # Classify and downsample\n",
    "                    background = classify_background(im)\n",
    "                    size=(1500, 1000)\n",
    "                    im = cv2.resize(im, size)\n",
    "                else:\n",
    "                    # Manually assign background\n",
    "                    background = 'neutral'\n",
    "\n",
    "                images.append(im)\n",
    "                ids.append(img_id)\n",
    "                backgrounds.append(background)\n",
    "\n",
    "    return images, ids, backgrounds\n",
    "\n",
    "images1, ids1, backgrounds1 = load_images_and_backgrounds(folder1, downsampled=True)\n",
    "images2, ids2, backgrounds2 = load_images_and_backgrounds(folder2, downsampled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(1)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor(2)\n",
      "tensor(0)\n",
      "neutral\n",
      "tensor(0)\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "def predict(images, ids, backgrounds):\n",
    "    predictions = {}\n",
    "    subclasses = {}\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    for i, (background, img) in enumerate(zip(backgrounds,images)):\n",
    "        print(background)\n",
    "        cropped_images, circles = detect_and_crop_coins(background, np.array(img))\n",
    "\n",
    "        pred = np.zeros(3)\n",
    "        subclass = np.zeros(16)\n",
    "        outputs = []\n",
    "\n",
    "        if len(cropped_images) != 0:\n",
    "            for coin, circle in zip(cropped_images, circles):\n",
    "                new_circle = detect_circles_classification(coin)\n",
    "                coin = preprocess(coin)\n",
    "                coin = coin.unsqueeze(0)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model1(coin)\n",
    "                output = torch.argmax(outputs) # index 0 -> CHF, index 1 -> EUR, index 2 -> OOD\n",
    "                print(output)\n",
    "                \n",
    "                if output == 0:\n",
    "                    area = circle_area(new_circle)\n",
    "                    label = classify_area(area)\n",
    "                \n",
    "                if output == 1:\n",
    "                    with torch.no_grad():\n",
    "                        outputs2 = model2(coin) # index 0 -> 0.01EUR, ..., index 7 -> 2EUR\n",
    "                    inverted_label = 7 - torch.argmax(outputs2).item() # index 0 -> 2EUR, ..., index 7 -> 0.01EUR\n",
    "                    label = 7 + inverted_label # index 7 -> 2EUR, ..., index 14 -> 0.01EUR\n",
    "\n",
    "                if output == 2:\n",
    "                    label = 15\n",
    "                \n",
    "                subclass[label] += 1 \n",
    "                subclasses[ids[i]] = subclass\n",
    "                pred[output] += 1 # add the coin to the image prediction\n",
    "\n",
    "        else:\n",
    "            print(f'no coins detected in image {i}')\n",
    "\n",
    "        predictions[ids[i]] = pred\n",
    "        \n",
    "    return predictions, subclasses\n",
    "\n",
    "predictions1, subclasses1 = predict(images1, ids1, backgrounds1)\n",
    "predictions2, subclasses2 = predict(images2, ids2, backgrounds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1010277': array([3., 0., 2.]),\n",
       " 'L1010279': array([2., 0., 2.]),\n",
       " 'L1010281': array([4., 0., 4.]),\n",
       " 'L1010283': array([6., 0., 0.]),\n",
       " 'L1010287': array([6., 0., 2.]),\n",
       " 'L1010288': array([4., 0., 1.]),\n",
       " 'L1010294': array([2., 0., 3.]),\n",
       " 'L1010297': array([3., 0., 1.]),\n",
       " 'L1010298': array([2., 0., 3.]),\n",
       " 'L1010300': array([4., 0., 1.]),\n",
       " 'L1010308': array([8., 0., 1.]),\n",
       " 'L1010310': array([7., 0., 0.]),\n",
       " 'L1010311': array([3., 0., 3.]),\n",
       " 'L1010318': array([6., 0., 5.]),\n",
       " 'L1010321': array([8., 0., 1.]),\n",
       " 'L1010323': array([8., 0., 0.]),\n",
       " 'L1010405': array([2., 1., 1.]),\n",
       " 'L1010406': array([3., 0., 2.]),\n",
       " 'L1010408': array([0., 0., 2.]),\n",
       " 'L1010410': array([1., 0., 2.]),\n",
       " 'L1010413': array([3., 0., 1.]),\n",
       " 'L1010418': array([1., 0., 1.]),\n",
       " 'L1010419': array([2., 0., 1.]),\n",
       " 'L1010421': array([3., 0., 1.]),\n",
       " 'L1010422': array([3., 0., 0.]),\n",
       " 'L1010424': array([5., 0., 0.]),\n",
       " 'L1010434': array([2., 0., 2.]),\n",
       " 'L1010436': array([1., 0., 0.]),\n",
       " 'L1010440': array([6., 0., 2.]),\n",
       " 'L1010445': array([2., 0., 1.]),\n",
       " 'L1010446': array([2., 0., 3.]),\n",
       " 'L1010450': array([3., 0., 2.]),\n",
       " 'L1010454': array([1., 0., 1.])}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1010277': array([2., 0., 3.]),\n",
       " 'L1010279': array([0., 0., 4.]),\n",
       " 'L1010281': array([3., 0., 5.]),\n",
       " 'L1010283': array([3., 0., 3.]),\n",
       " 'L1010287': array([5., 0., 3.]),\n",
       " 'L1010288': array([3., 0., 2.]),\n",
       " 'L1010294': array([2., 0., 3.]),\n",
       " 'L1010297': array([0., 0., 4.]),\n",
       " 'L1010298': array([1., 0., 4.]),\n",
       " 'L1010300': array([1., 0., 4.]),\n",
       " 'L1010308': array([8., 0., 1.]),\n",
       " 'L1010310': array([4., 0., 3.]),\n",
       " 'L1010311': array([3., 0., 3.]),\n",
       " 'L1010318': array([5., 0., 6.]),\n",
       " 'L1010321': array([5., 0., 4.]),\n",
       " 'L1010323': array([7., 0., 1.]),\n",
       " 'L1010405': array([2., 1., 1.]),\n",
       " 'L1010406': array([3., 0., 2.]),\n",
       " 'L1010408': array([0., 0., 2.]),\n",
       " 'L1010410': array([1., 0., 2.]),\n",
       " 'L1010413': array([2., 0., 2.]),\n",
       " 'L1010418': array([1., 0., 1.]),\n",
       " 'L1010419': array([2., 0., 1.]),\n",
       " 'L1010421': array([2., 0., 2.]),\n",
       " 'L1010422': array([3., 0., 0.]),\n",
       " 'L1010424': array([2., 0., 3.]),\n",
       " 'L1010434': array([2., 0., 2.]),\n",
       " 'L1010436': array([0., 0., 1.]),\n",
       " 'L1010440': array([6., 0., 2.]),\n",
       " 'L1010445': array([1., 0., 2.]),\n",
       " 'L1010446': array([2., 0., 3.]),\n",
       " 'L1010450': array([3., 0., 2.]),\n",
       " 'L1010454': array([1., 0., 1.])}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "subclasses = subclasses1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 17)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "#df.drop(df.columns[8:17], axis=1, inplace=True)\n",
    "\n",
    "selected_rows = df[df['id'].isin(subclasses.keys())]\n",
    "selected_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1010277\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0]\n",
      "[0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010279\n",
      "[0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010281\n",
      "[0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0]\n",
      "[0. 0. 2. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 4.]\n",
      "L1010283\n",
      "[0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0]\n",
      "[0. 1. 4. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "L1010287\n",
      "[0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 0]\n",
      "[0. 0. 2. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010288\n",
      "[0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "[0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010294\n",
      "[1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]\n",
      "L1010297\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "[0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010298\n",
      "[0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]\n",
      "L1010300\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0]\n",
      "[0. 0. 3. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010308\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 2, 2, 0, 0]\n",
      "[0. 1. 4. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010310\n",
      "[0, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "[0. 2. 2. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "L1010311\n",
      "[0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]\n",
      "[0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]\n",
      "L1010318\n",
      "[1, 1, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 0, 0, 2, 0]\n",
      "[0. 0. 1. 0. 1. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 5.]\n",
      "L1010321\n",
      "[0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 1, 0, 0, 0, 0]\n",
      "[0. 1. 5. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010323\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0. 0. 5. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "L1010405\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1]\n",
      "[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "L1010406\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1]\n",
      "[0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010408\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010410\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010413\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2]\n",
      "[0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010418\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010419\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "[0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010421\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "[0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010422\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2]\n",
      "[0. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "L1010424\n",
      "[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0. 0. 0. 1. 1. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "L1010434\n",
      "[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "[0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010436\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "L1010440\n",
      "[0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1]\n",
      "[0. 0. 3. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010445\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2]\n",
      "[0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "L1010446\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2]\n",
      "[0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]\n",
      "L1010450\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2]\n",
      "[0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "L1010454\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "Total accuracy: 66.86%\n",
      "F1 score: 0.2974\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store total correct predictions and total elements\n",
    "total_correct = 0\n",
    "total_elements = 0\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "\n",
    "# Iterate over each key in subclasses\n",
    "for key in subclasses.keys():\n",
    "    # Extract the row values from the DataFrame\n",
    "    row_values = df.loc[df['id'] == key].iloc[0, 1:17].tolist()\n",
    "\n",
    "    # Count the number of element-wise equal elements\n",
    "    equal_count = sum(1 for x, y in zip(row_values, subclasses[key]) if x == y)\n",
    "\n",
    "    # Increment total correct and total elements\n",
    "    total_correct += equal_count\n",
    "    total_elements += len(subclasses[key])\n",
    "\n",
    "    print(key)\n",
    "    print(row_values)\n",
    "    print(subclasses[key])\n",
    "\n",
    "    # Collect true labels and predicted labels for F1 score computation\n",
    "    all_true_labels.append(subclasses[key])\n",
    "    all_pred_labels.append(row_values)\n",
    "\n",
    "# Calculate total accuracy\n",
    "total_accuracy = (total_correct / total_elements) * 100\n",
    "\n",
    "# Compute F1 score using the custom function\n",
    "f1 = compute_f1(all_pred_labels, all_true_labels)\n",
    "\n",
    "print(f\"Total accuracy: {total_accuracy:.2f}%\")\n",
    "print(f\"F1 score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
